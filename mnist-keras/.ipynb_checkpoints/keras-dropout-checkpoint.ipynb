{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.9876 - acc: 0.8337 - val_loss: 0.3853 - val_acc: 0.9337\n",
      "Epoch 2/3\n",
      " 3712/48000 [=>............................] - ETA: 2s - loss: 0.4693 - acc: 0.9017"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers \n",
    "from keras.models import load_model\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 3\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = Adam() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback): \n",
    "    def on_train_begin(self, logs={}): \n",
    "        self.losses = [] \n",
    "    def on_batch_end(self, batch, logs={}): \n",
    "        self.losses.append(logs.get('loss')) \n",
    "\n",
    "losshistory = LossHistory() \n",
    "         \n",
    "model = Sequential()\n",
    "\n",
    "#model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Dense(N_HIDDEN, input_dim=RESHAPED, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT, callbacks=[losshistory])\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.590208, 4.51606, 4.4054365, 4.2632346, 4.2001762, 4.1417866, 4.035789, 3.9415793, 3.8281798, 3.7662477, 3.6184683, 3.5994596, 3.4574769, 3.2900681, 3.3050008, 3.1536236, 3.1006966, 3.0163994, 2.8817804, 2.8623781, 2.8874855, 2.6278982, 2.6045735, 2.5185041, 2.5026655, 2.4587457, 2.289219, 2.2689059, 2.2462502, 2.1271281, 2.2464883, 2.1263494, 2.045902, 1.9623852, 1.9386005, 1.9170691, 1.9298263, 1.7624927, 1.8348415, 1.8457549, 1.9066886, 1.8569894, 1.7279805, 1.7259443, 1.8506134, 1.6953521, 1.5705861, 1.7330967, 1.4595107, 1.5763755, 1.6498885, 1.4402299, 1.4793717, 1.4489499, 1.568613, 1.389533, 1.3893844, 1.3718543, 1.2450691, 1.2685795, 1.4159882, 1.3594161, 1.2960076, 1.4663348, 1.2525749, 1.1594558, 1.2890081, 1.297997, 1.3431637, 1.2675899, 1.2377045, 1.107575, 1.1606065, 1.1115439, 0.944294, 1.0547502, 1.2395248, 1.1413902, 1.3078971, 1.0965998, 1.0200171, 1.1638843, 1.0644343, 1.1551032, 1.2105583, 1.1062036, 1.2344325, 1.1213727, 1.230884, 1.0034257, 1.1539222, 0.9810957, 0.9602362, 1.0064299, 1.0335293, 0.86627865, 1.051549, 1.0484468, 1.0927093, 0.94453573, 0.956532, 1.0399454, 0.9024595, 0.92120624, 0.9328952, 1.0124798, 0.9416684, 1.1018159, 0.8923346, 1.0720148, 0.8111248, 1.0041045, 0.85375214, 0.95383424, 0.8573284, 0.833905, 0.83383304, 0.8037008, 0.9940655, 0.8124728, 0.7697039, 0.99318063, 0.7907361, 0.7775247, 0.880115, 0.77413285, 0.7509457, 0.89530766, 0.78658307, 0.75605714, 0.84396976, 0.80892533, 0.7781807, 0.7289169, 0.850629, 0.7870625, 0.81548774, 0.7617595, 0.82593876, 0.7854682, 0.73345697, 0.77179193, 0.86306036, 0.7175518, 0.7663481, 0.85364425, 0.69475967, 0.85647154, 0.8880861, 0.73683035, 0.7356344, 0.82589686, 0.71681285, 0.7183205, 0.8070528, 0.8879367, 0.76485527, 0.87400025, 0.62458515, 0.65216184, 0.7215699, 0.71769935, 0.753487, 0.82520974, 0.7622391, 0.69787544, 0.7538616, 0.72078514, 0.7916354, 0.8351326, 0.69435954, 0.62430555, 0.63972616, 0.6922722, 0.60409904, 0.80040264, 0.6703204, 0.7845552, 0.6101755, 0.5520252, 0.7859103, 0.7848097, 0.6999575, 0.6940402, 0.7623228, 0.6749425, 0.688516, 0.60152304, 0.7436945, 0.7664901, 0.6473719, 0.7306261, 0.6437664, 0.67293346, 0.64978004, 0.6288601, 0.6089926, 0.5921812, 0.7971782, 0.5857283, 0.5737593, 0.57978696, 0.57891893, 0.5433829, 0.6716406, 0.7007562, 0.5213771, 0.5788164, 0.7231375, 0.6021911, 0.69957924, 0.7038144, 0.7030254, 0.6268697, 0.5623579, 0.5853143, 0.6275482, 0.57782346, 0.5736838, 0.53056717, 0.6623614, 0.59858996, 0.5415769, 0.5138649, 0.7673759, 0.7476171, 0.6089251, 0.55716896, 0.6618887, 0.6194141, 0.7085659, 0.6759708, 0.5285801, 0.68915486, 0.57377493, 0.5742432, 0.6449033, 0.69879234, 0.5281985, 0.5936467, 0.54923314, 0.6386414, 0.5107805, 0.60383815, 0.6666672, 0.6011317, 0.56382376, 0.5638581, 0.54901266, 0.6109909, 0.6843851, 0.5594152, 0.5772656, 0.6708108, 0.6363104, 0.5706774, 0.57591486, 0.5884374, 0.44809425, 0.48995212, 0.57997715, 0.5865747, 0.70319605, 0.5936731, 0.56199557, 0.62931037, 0.65012944, 0.5534283, 0.48562732, 0.4541412, 0.54309595, 0.5976028, 0.5395087, 0.50518095, 0.4842642, 0.6134261, 0.57305646, 0.58879966, 0.6541421, 0.527088, 0.55455774, 0.65273815, 0.41230196, 0.41724718, 0.5567657, 0.58820367, 0.59867394, 0.58958805, 0.5366233, 0.50477785, 0.6906523, 0.54291356, 0.4765008, 0.52186304, 0.524545, 0.69080806, 0.55257976, 0.5993982, 0.50380266, 0.6782655, 0.44230855, 0.42707402, 0.5211085, 0.47885272, 0.4734589, 0.50167036, 0.49051696, 0.4295448, 0.48378876, 0.49316525, 0.5326995, 0.6153177, 0.5324528, 0.5254881, 0.6158104, 0.5619657, 0.5124761, 0.43778354, 0.4768452, 0.5357712, 0.5077877, 0.46199942, 0.53575766, 0.52173865, 0.5068613, 0.5670329, 0.45464233, 0.52445555, 0.46750453, 0.6667298, 0.5551345, 0.5558634, 0.51969844, 0.59344316, 0.5319201, 0.4080932, 0.43111107, 0.5572958, 0.49400842, 0.52732813, 0.60422397, 0.49654445, 0.515618, 0.5501977, 0.40622517, 0.49776363, 0.40584427, 0.4271351, 0.567279, 0.6194311, 0.4871906, 0.45092005, 0.50058156, 0.4757731, 0.4748668, 0.5060903, 0.49076277, 0.5883814, 0.4212388, 0.57556355, 0.4513411, 0.47303128, 0.57985246, 0.5170845, 0.5043777, 0.6287703, 0.58210236, 0.46011594, 0.46463668, 0.45745274, 0.4277151, 0.55776453, 0.47222036, 0.43345058, 0.54576766, 0.5323103, 0.46616608, 0.3607791, 0.5271123, 0.46787345, 0.4808117, 0.48223865, 0.4772966, 0.4533573, 0.57273734, 0.4405344, 0.53306216, 0.5556299, 0.45994574, 0.54742146, 0.49489105, 0.4322775, 0.4345293, 0.46953225, 0.49700156, 0.42726803, 0.46163857, 0.48763132, 0.3764034, 0.4412781, 0.5236678, 0.3240292, 0.48246926, 0.4647206, 0.4046116, 0.4940996, 0.59145516, 0.56217027, 0.50309175, 0.4731441, 0.5876129, 0.5293673, 0.44254535, 0.40805528, 0.49038494, 0.48591012, 0.47879505, 0.5139483, 0.421846, 0.4942497, 0.45304665, 0.52186936, 0.6442883, 0.45265296, 0.41952038, 0.47132504, 0.46297514, 0.47850537, 0.41162148, 0.51959264, 0.34137887, 0.4599938, 0.44428366, 0.56463224, 0.46726358, 0.36999604, 0.36813208, 0.3854664, 0.44578865, 0.4396776, 0.41622633, 0.44214857, 0.41658777, 0.5266106, 0.43179128, 0.4041763, 0.46528077, 0.39866152, 0.44132918, 0.33861208, 0.49163693, 0.4581588, 0.6182424, 0.4591782, 0.37982696, 0.40817738, 0.49146065, 0.49536163, 0.47748226, 0.4217017, 0.32926822, 0.39014482, 0.41620725, 0.41935545, 0.3728006, 0.5924134, 0.47383767, 0.37862158, 0.47247827, 0.5051354, 0.43184924, 0.48408306, 0.41493046, 0.44697997, 0.37280035, 0.48183507, 0.4967997, 0.4381321, 0.4522081, 0.50485885, 0.45746878, 0.4559754, 0.41624624, 0.3590287, 0.36701092, 0.4993714, 0.42842638, 0.5959749, 0.41963798, 0.3754822, 0.3938843, 0.43405017, 0.44804275, 0.3592899, 0.42949283, 0.47221667, 0.4868396, 0.39363787, 0.41066694, 0.5107621, 0.4599105, 0.3689083, 0.44418895, 0.45819947, 0.3551052, 0.40855512, 0.3936935, 0.46874586, 0.34278345, 0.35311258, 0.43304387, 0.43283117, 0.64139354, 0.40636915, 0.41889718, 0.4229107, 0.533038, 0.39864522, 0.43258947, 0.44691026, 0.44133532, 0.4653265, 0.44159484, 0.5556525, 0.36344045, 0.5091706, 0.4809227, 0.50107336, 0.45166835, 0.4112733, 0.33256155, 0.4994354, 0.48577294, 0.32798836, 0.4067129, 0.34273916, 0.47315943, 0.42860198, 0.4951886, 0.512769, 0.37317967, 0.42583042, 0.40533108, 0.5382771, 0.44100648, 0.38048065, 0.54136753, 0.40534222, 0.34490338, 0.4009124, 0.35871726, 0.54601806, 0.37657115, 0.41379476, 0.43751577, 0.5574249, 0.46633172, 0.524238, 0.42548987, 0.4985426, 0.46123958, 0.44109154, 0.4591539, 0.47119114, 0.43858376, 0.36795574, 0.43693125, 0.48001248, 0.3355453, 0.35215253, 0.4035896, 0.56328356, 0.44519228, 0.36492258, 0.35050339, 0.35349315, 0.44633555, 0.3603961, 0.40366465, 0.4486971, 0.3559274, 0.41270417, 0.34914634, 0.31129175, 0.49206376, 0.42333397, 0.4139974, 0.26011807, 0.40759903, 0.41489142, 0.45701385, 0.40653256, 0.40886086, 0.37744802, 0.3192644, 0.39682233, 0.47232208, 0.5135951, 0.47599328, 0.43000197, 0.3584454, 0.38615918, 0.52340984, 0.47402823, 0.43840975, 0.38483393, 0.30037588, 0.40064976, 0.42483354, 0.3880657, 0.433984, 0.3036794, 0.42282844, 0.43190843, 0.3342082, 0.53596365, 0.36518547, 0.4334244, 0.46099642, 0.37383223, 0.38087305, 0.4156708, 0.4518964, 0.35356733, 0.40179247, 0.4565788, 0.39961088, 0.50076205, 0.4793173, 0.35223067, 0.37055522, 0.36339438, 0.3525199, 0.38228744, 0.44875768, 0.33453512, 0.29901472, 0.46857533, 0.4063576, 0.4857441, 0.61016524, 0.4054685, 0.53824735, 0.29560536, 0.45571938, 0.45333266, 0.37476796, 0.3280441, 0.34343705, 0.367432, 0.3506288, 0.38440347, 0.41547662, 0.37844986, 0.33473173, 0.3876309, 0.36516476, 0.3959481, 0.32733107, 0.41609466, 0.5081392, 0.45938313, 0.38110763, 0.35375637, 0.39657372, 0.29176772, 0.48092005, 0.40522528, 0.42182374, 0.44232067, 0.5941263, 0.25720027, 0.41395003, 0.34727955, 0.48757514, 0.37314802, 0.4456886, 0.35801998, 0.5177864, 0.33598223, 0.36208317, 0.48060918, 0.430084, 0.559722, 0.3252996, 0.44981244, 0.556057, 0.35416484, 0.38730747, 0.3642746, 0.4365223, 0.34567803, 0.37224823, 0.33733368, 0.3388026, 0.41174904, 0.38089716, 0.38773462, 0.36921275, 0.40220118, 0.45048532, 0.3309082, 0.4731787, 0.28964126, 0.31132585, 0.3803913, 0.33343613, 0.44091508, 0.43040043, 0.39447996, 0.43917412, 0.2882264, 0.48517078, 0.44469202, 0.34703422, 0.49209452, 0.40532643, 0.4093038, 0.3085114, 0.38087654, 0.28044003, 0.3456261, 0.4005314, 0.40796334, 0.44746858, 0.3520393, 0.3637243, 0.36721906, 0.34746695, 0.4371958, 0.4097163, 0.39924723, 0.4216256, 0.351049, 0.35030633, 0.4626977, 0.41463166, 0.3628425, 0.3322298, 0.41200754, 0.33115092, 0.39305693, 0.40139258, 0.3659156, 0.36412472, 0.34638804, 0.3717479, 0.4473804, 0.3745224, 0.49154785, 0.5080757, 0.39284065, 0.283154, 0.33583456, 0.3413554, 0.37047032, 0.34179354, 0.3623481, 0.31088278, 0.43948013, 0.35016584, 0.35810918, 0.39763367, 0.43303776, 0.38957474, 0.33666867, 0.4559679, 0.4075347, 0.3827266, 0.4125562, 0.43816167, 0.4148609, 0.438056, 0.3077011, 0.34029543, 0.31055218, 0.36442754, 0.37744018, 0.41167915, 0.30746433, 0.37465715, 0.45537895, 0.4706217, 0.31310266, 0.4411456, 0.31158146, 0.2928863, 0.27941394, 0.37141782, 0.31091624, 0.31926632, 0.42709085, 0.4394484, 0.37088132, 0.45750347, 0.33027852, 0.41997027, 0.3092271, 0.4086941, 0.37597567, 0.4105268, 0.38112515, 0.34491986, 0.39680246, 0.46882087, 0.41689312, 0.40025955, 0.3514573, 0.41759652, 0.45119497, 0.4069528, 0.3771597, 0.37739033, 0.35767, 0.39527953, 0.4040675, 0.48540455, 0.5365038, 0.40306598, 0.3525631, 0.4033523, 0.31342122, 0.4658742, 0.34897655, 0.31572697, 0.3827178, 0.45565072, 0.28944832, 0.41131425, 0.48489162, 0.42420796, 0.405191, 0.26627478, 0.35518485, 0.35909185, 0.33939093, 0.36997455, 0.31206858, 0.31566146, 0.3603865, 0.34738308, 0.39565364, 0.3485641, 0.33341688, 0.36518985, 0.3449211, 0.39814687, 0.30879584, 0.4328176, 0.43828782, 0.38726044, 0.3128766, 0.31486094, 0.30808204, 0.40314284, 0.3495669, 0.31984732, 0.4716295, 0.37464905, 0.3551371, 0.39325, 0.4869904, 0.31296128, 0.38915193, 0.39375743, 0.46170864, 0.38695508, 0.3549928, 0.39095336, 0.37338755, 0.3428762, 0.37412107, 0.376343, 0.3729153, 0.41923314, 0.33487818, 0.34902376, 0.4443939, 0.45194724, 0.42949426, 0.2386624, 0.3564094, 0.29025826, 0.416538, 0.3130792, 0.34297404, 0.33940232, 0.3628651, 0.33126625, 0.41167176, 0.42031872, 0.39067495, 0.40290642, 0.4675911, 0.40881458, 0.33606702, 0.3857655, 0.3919616, 0.40039468, 0.35129437, 0.3646224, 0.42835456, 0.3995059, 0.46789575, 0.39819023, 0.39582914, 0.4093759, 0.25151277, 0.3408919, 0.3249213, 0.33815444, 0.38563415, 0.32086772, 0.5358261, 0.4257702, 0.43372428, 0.45381558, 0.4292035, 0.37477446, 0.49683493, 0.4521042, 0.29145855, 0.42012984, 0.286577, 0.35839975, 0.40766555, 0.2910926, 0.28718346, 0.4226654, 0.3325078, 0.32663786, 0.43019554, 0.42511678, 0.3882832, 0.38932896, 0.45896637, 0.35604826, 0.35224572, 0.41028804, 0.41650134, 0.34898102, 0.47014183, 0.33301306, 0.3617732, 0.40359777, 0.33260763, 0.36437654, 0.300453, 0.39436835, 0.5109938, 0.35457546, 0.37091345, 0.32983208, 0.3403679, 0.4320168, 0.3379836, 0.34956208, 0.41956723, 0.3560893, 0.39171338, 0.3048588, 0.40817773, 0.41813383, 0.40785685, 0.4393581, 0.37869805, 0.29379204, 0.41687346, 0.3712608, 0.48019844, 0.4967304, 0.40045553, 0.42879236, 0.47242472, 0.38105893, 0.49855387, 0.40172577, 0.2909617, 0.33661586, 0.3037914, 0.3959242, 0.47876537, 0.28225106, 0.3146865, 0.39011806, 0.3616079, 0.35677543, 0.28090614, 0.36415118, 0.38657767, 0.38653874, 0.36265117, 0.38533154, 0.4608639, 0.38625258, 0.40367794, 0.632468, 0.41304687, 0.3965569, 0.44371778, 0.33623254, 0.27361527, 0.503073, 0.3981427, 0.34385324, 0.45657504, 0.2974984, 0.36389434, 0.41149133, 0.45191693, 0.30582097, 0.33275664, 0.39115617, 0.2879011, 0.41122323, 0.37179944, 0.2694415, 0.31805217, 0.3310355, 0.33595574, 0.3175854, 0.4286541, 0.42422435, 0.37105846, 0.31275213, 0.45229217, 0.2992576, 0.27250016, 0.41951352, 0.36926425, 0.32197064, 0.28824073, 0.3979914, 0.2777812, 0.35287672, 0.37943158, 0.33852327, 0.28345385, 0.34725848, 0.37835613, 0.33820796, 0.3036961, 0.39493948, 0.27863067, 0.45417172, 0.33584398, 0.31134114, 0.36637235, 0.38701707, 0.3029439, 0.34020883, 0.3719368, 0.35826302, 0.28656727, 0.2590637, 0.4093826, 0.32652372, 0.28717917, 0.35041064, 0.3883874, 0.33870018, 0.3751177, 0.30812535, 0.41000813, 0.3406553, 0.42763093, 0.4180895, 0.33732808, 0.36983752, 0.4023698, 0.34262207, 0.40014702, 0.331559, 0.38276762, 0.46310222, 0.34257773, 0.28447682, 0.3356654, 0.24520296, 0.37879598, 0.37864012, 0.36957633, 0.39743516, 0.36564863, 0.44434148, 0.3132354, 0.39540488, 0.3337465, 0.3509519, 0.34379694, 0.4589696, 0.28874516, 0.32891434, 0.30479175, 0.42145634, 0.2944615, 0.32953167, 0.4424519, 0.37214392, 0.29717037, 0.30040914, 0.34200776, 0.41255328, 0.36418667, 0.31893086, 0.43578678, 0.41325167, 0.4445818, 0.35790288, 0.24903572, 0.42618573, 0.34903762, 0.37054268, 0.35032243, 0.33896327, 0.45541093, 0.31932768, 0.37565377, 0.3330633, 0.28405413, 0.32049978, 0.28526476, 0.3266402, 0.4084891, 0.34169838]\n"
     ]
    }
   ],
   "source": [
    "print(losshistory.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "model.predict_proba(X_test)[0]\n",
    "model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')\n",
    "# creates a HDF5 file 'my_model.h5' del model\n",
    "# deletes the existing model\n",
    "# returns a compiled model\n",
    "# identical to the previous one model = load_model('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
